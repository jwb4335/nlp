{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "hw1_part1_johnbarry_20200125.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwb4335/nlp/blob/master/hw_part1/hw1_part1_johnbarry_20200125.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHA-uUPMFzkJ",
        "colab_type": "text"
      },
      "source": [
        "## HW1 Part 1\n",
        "John Barry\n",
        "COMPSCI 590.07\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ULrjiyxFri5",
        "colab_type": "text"
      },
      "source": [
        "New topics in SLP3 are:\n",
        "- Neural networks and neural language models\n",
        "- Why: Neural networks have pushed to the forefront of machine learning and natural language processing. They have to be there\n",
        "- Sequence processing with recurrent networks\n",
        "- Why: same reason\n",
        "- Encoder-decoder models and contextual embeddings\n",
        "- Why: Another example neural networks\n",
        "- Constituency grammar\n",
        "- Why: It is natural to think of language in grammatical units as opposed to words. This is probably not new but just an update of the syntax section in SLP2. \n",
        "\n",
        "Retired topics from SLP2 are:\n",
        "- Hidden Markov Models (actually moved to appendix)\n",
        "- Why: Been overtaken by neural networks\n",
        "- Phonetics and Speech synthesis(actually moved down a lot)\n",
        "- Why: I don't know, this is more to do with speech I guess. Maybe machines are bad at phonetic parsing\n",
        "- Computational phonology\n",
        "- Why: Maybe because it's less interesting.\n",
        "- Lexical semantics:\n",
        "- Why: Lexical semantics focuses on breaking down words into lexical units. As machine learning in NLP relies on context, it makes more sense to build up into grammatical or semantical units of collocoated words. I guess?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8_n8WYHFqy3",
        "colab_type": "code",
        "outputId": "850632db-5861-4bc3-efe8-60b5f1841ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#!pip3 install --quiet nltk \n",
        "#!pip3 install --quiet textract\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import textract\n",
        "import nltk\n",
        "import urllib\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "## Load in the SLP2 and SLP3 toc's\n",
        "## NOTE: The blotted-out code converts the PDFs to text. The urls are github links to those textfiles\n",
        "\"\"\"\n",
        "slp2 = textract.process('Table of Contents slp-2.pdf').decode('utf-8')\n",
        "slp3 = textract.process('Table of Contents slp-3.pdf').decode('utf-8')\n",
        "slp2 = '#' + slp2\n",
        "spl3 = '#' + slp3\n",
        "with open('slp2.txt', 'w') as f:\n",
        "    f.write(slp2)\n",
        "with open('slp3.txt', 'w') as f:\n",
        "    f.write(slp3)\n",
        "\"\"\"\n",
        "url2 = 'https://raw.githubusercontent.com/jwb4335/nlp/master/hw1_part1/slp2.txt'\n",
        "url3 = 'https://raw.githubusercontent.com/jwb4335/nlp/master/hw1_part1/slp3.txt'\n",
        "\n",
        "slp2 = urllib.request.urlopen(url2).read().decode('utf-8')\n",
        "slp3 = urllib.request.urlopen(url3).read().decode('utf-8')\n",
        "\n",
        "## Store the objects for the loop\n",
        "contents = [slp2,slp3]\n",
        "\n",
        "## Create a store matrix for the bigrams\n",
        "bigrams = [[''],['']]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReFVkdvlFqy6",
        "colab_type": "code",
        "outputId": "bbdcfb55-dd34-4df2-bf87-9b93a15c2991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "## We need to clean the toc's a little. Remove from non-interesting words, numbers and symbols\n",
        "i = 0\n",
        "for toc in contents:\n",
        "    toc = toc.replace('. ','')\n",
        "    toc = ''.join([i for i in toc if not i.isdigit()])\n",
        "    toc = toc.replace('Summary of Contents','')\n",
        "    toc = toc.replace('Preface .xxiii','')\n",
        "    toc = toc.replace('Introduction','')\n",
        "    toc = toc.replace('Bibliography','')\n",
        "    toc = toc.replace('Index','')\n",
        "    toc = toc.replace('vii','')\n",
        "    toc = toc.replace('I Words','')\n",
        "    toc = toc.replace('II Speech','')\n",
        "    toc = toc.replace('III Syntax','')\n",
        "    toc = toc.replace('IV Semantics and Pragmatics','')\n",
        "    toc = toc.replace('Applications','')\n",
        "    toc = toc.replace('\\nV','')\n",
        "    toc = toc.replace('\\nD','',1)\n",
        "    toc = toc.replace('\\nRA','')\n",
        "    toc = toc.replace('\\nFT','')\n",
        "    toc = toc.replace('&','and')\n",
        "    toc = toc.replace(':',' ')\n",
        "    toc = toc.replace(',',' ')   \n",
        "    toc = toc.replace('#','')\n",
        "    token = word_tokenize(toc)\n",
        "    bigrams[i] = list(ngrams(token,2))\n",
        "    i = i+1\n",
        "\"\"\" \n",
        "print('The bigram for SLP2:')\n",
        "for line in bigrams[0]:\n",
        "    print(line)\n",
        "    \n",
        "print('\\n\\n')\n",
        "print('The bigram for SLP3:')\n",
        "for line in bigrams[1]:\n",
        "    print(line)\n",
        "\"\"\""
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\nprint('The bigram for SLP2:')\\nfor line in bigrams[0]:\\n    print(line)\\n    \\nprint('\\n\\n')\\nprint('The bigram for SLP3:')\\nfor line in bigrams[1]:\\n    print(line)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCs2FqxePl4S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7b5d85f0-233a-4841-c34f-9fdc63111185"
      },
      "source": [
        "\"\"\"def ngram_cos_sim(ngram_1,ngram_2):\n",
        "  # Get the n-gram counts of each vector\n",
        "  vec_1_dict = dict(Counter(ngram_1))\n",
        "  vec_2_dict = dict(Counter(ngram_2))\n",
        "  a = np.fromiter(.values(),dtype = 'float')\n",
        "  b = np.fromiter(.values(),dtype = 'float')\"\"\"\n",
        "ngram_1 = sorted(bigrams[0])\n",
        "ngram_2 = sorted(bigrams[1])\n",
        "ngram_all = sorted(list(set(ngram_1 + ngram_2)))\n",
        "vec_1 = Counter(ngram_1)\n",
        "vec_2 = Counter(ngram_2)\n",
        "print('Here are the bigrams that appear in both TOCs:\\n{}'.format(ngram_all))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here are the bigrams that appear in both TOCs:\n",
            "[('A', 'Hidden'), ('Advanced', 'Topics'), ('Affect', 'and'), ('Agents', 'Machine'), ('Answering', 'Dialogue'), ('Answering', 'and'), ('Appendix', 'A'), ('Attention', 'and'), ('Automata', 'Words'), ('Automatic', 'Speech'), ('Bayes', 'and'), ('Chatbots', 'Phonetics'), ('Classification', 'Logistic'), ('Coherence', 'Summarization'), ('Complexity', 'Representing'), ('Computational', 'Discourse'), ('Computational', 'Lexical'), ('Computational', 'Phonology'), ('Computational', 'Semantics'), ('Connotation', 'Coreference'), ('Constituency', 'Grammars'), ('Constituency', 'Parsing'), ('Context-Free', 'Grammars'), ('Contextual', 'Embeddings'), ('Conversational', 'Agents'), ('Coreference', 'Resolution'), ('Dialogue', 'Systems'), ('Dialogue', 'and'), ('Discourse', 'Coherence'), ('Discourse', 'Information'), ('Distance', 'N-gram'), ('Edit', 'Distance'), ('Embeddings', 'Machine'), ('Embeddings', 'Neural'), ('Encoder-Decoder', 'Models'), ('English', 'Parsing'), ('Entropy', 'Models'), ('Expressions', 'Text'), ('Expressions', 'and'), ('Extraction', 'Question'), ('Extraction', 'Word'), ('Features', 'and'), ('Formal', 'Grammars'), ('Grammars', 'Constituency'), ('Grammars', 'Statistical'), ('Grammars', 'of'), ('Hidden', 'Markov'), ('Information', 'Extraction'), ('Labeling', 'Lexicons'), ('Language', 'Models'), ('Language', 'and'), ('Lexical', 'Semantics'), ('Lexicons', 'for'), ('Logical', 'Representations'), ('Logistic', 'Regression'), ('Machine', 'Translation'), ('Markov', 'Models'), ('Markov', 'and'), ('Maximum', 'Entropy'), ('Meaning', 'Computational'), ('Models', 'Attention'), ('Models', 'Naive'), ('Models', 'Part-of-Speech'), ('Models', 'Phonetics'), ('N-gram', 'Language'), ('N-grams', 'Part-of-Speech'), ('Naive', 'Bayes'), ('Networks', 'Encoder-Decoder'), ('Networks', 'and'), ('Neural', 'Language'), ('Neural', 'Networks'), ('Normalization', 'Edit'), ('Parsing', 'Features'), ('Parsing', 'Information'), ('Parsing', 'Logical'), ('Parsing', 'Statistical'), ('Parsing', 'ependency'), ('Parsing', 'with'), ('Part-of-Speech', 'Tagging'), ('Phonetics', 'Speech'), ('Phonology', 'Formal'), ('Processing', 'with'), ('Question', 'Answering'), ('Recognition', 'Advanced'), ('Recognition', 'Speech'), ('Recognition', 'and'), ('Recurrent', 'Networks'), ('Regression', 'ector'), ('Regular', 'Expressions'), ('Representations', 'of'), ('Representing', 'Meaning'), ('Resolution', 'Discourse'), ('Role', 'Labeling'), ('Semantic', 'Parsing'), ('Semantic', 'Role'), ('Semantics', 'Computational'), ('Semantics', 'Lexical'), ('Semantics', 'and'), ('Senses', 'and'), ('Sentence', 'Meaning'), ('Sentiment', 'Affect'), ('Sentiment', 'Classification'), ('Sequence', 'Processing'), ('Speech', 'Recognition'), ('Speech', 'Synthesis'), ('Statistical', 'Constituency'), ('Statistical', 'Parsing'), ('Summarization', 'Dialogue'), ('Summarization', 'Question'), ('Synthesis', 'Appendix'), ('Synthesis', 'Automatic'), ('Systems', 'and'), ('Tagging', 'Hidden'), ('Tagging', 'Sequence'), ('Text', 'Normalization'), ('Topics', 'Computational'), ('Transducers', 'N-grams'), ('Translation', 'Constituency'), ('Unification', 'Language'), ('Word', 'Senses'), ('WordNet', 'Semantic'), ('Words', 'and'), ('and', 'Automata'), ('and', 'Chatbots'), ('and', 'Complexity'), ('and', 'Connotation'), ('and', 'Contextual'), ('and', 'Conversational'), ('and', 'Embeddings'), ('and', 'Maximum'), ('and', 'Neural'), ('and', 'Semantic'), ('and', 'Sentiment'), ('and', 'Summarization'), ('and', 'Synthesis'), ('and', 'Transducers'), ('and', 'Unification'), ('and', 'WordNet'), ('ector', 'Semantics'), ('ependency', 'Parsing'), ('for', 'Sentiment'), ('of', 'English'), ('of', 'Sentence'), ('with', 'Context-Free'), ('with', 'Recurrent')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nuutgVUXou-",
        "colab_type": "code",
        "outputId": "7e62a1fd-13aa-45b6-e40b-b12e05c46731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## This defines the cosine similarity measure. \n",
        "def cos_sim_count(c1, c2):\n",
        "    terms = set(c1).union(c2)\n",
        "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
        "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
        "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
        "    return dotprod / (magA * magB)\n",
        "\n",
        "cos_sim = round(cos_sim_count(vec_1,vec_2),4)\n",
        "\n",
        "print(\"\"\"The cosine similarity is {}. This is lower than the score I would give, it does show that the TOCs have changed a lot\"\"\".format(cos_sim))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cosine similarity is 0.129. This is lower than the score I would give, it does show that the TOCs have changed a lot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiDxDDDpgKms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}