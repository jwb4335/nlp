{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2part1_johnbarry_20200120.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwb4335/nlp/blob/master/hw2_part1/hw2part1_johnbarry_20200120.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lQYv-MZUXBk",
        "colab_type": "text"
      },
      "source": [
        "## Homework 2 part 1\n",
        "## CS 590.07\n",
        "## John Barry\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s04yG6eZ89-4",
        "colab_type": "text"
      },
      "source": [
        "## Load in simlex999 data from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaAWRcah9DYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from random import randint\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiV0AXKU9DeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/jwb4335/nlp/master/hw2_part1/Sim_Lex999.txt'\n",
        "simlex = pd.read_csv(url,sep = ':')\n",
        "simlex.columns = ['word1','word2','SimLex999','POS1','POS2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhzRQqVr-hZE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Let's get three random rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y3JCFlr-GFt",
        "colab_type": "code",
        "outputId": "acb01eee-7df8-4579-df5a-bfe01fd1b002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "randints = [randint(0,len(simlex)), randint(0,len(simlex)),randint(0,len(simlex))]\n",
        "\n",
        "simlex_rand = simlex.iloc[randints,:]\n",
        "simlex_rand\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>SimLex999</th>\n",
              "      <th>POS1</th>\n",
              "      <th>POS2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>934</th>\n",
              "      <td>reduce</td>\n",
              "      <td>shrink</td>\n",
              "      <td>8.02</td>\n",
              "      <td>v</td>\n",
              "      <td>v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>787</th>\n",
              "      <td>kill</td>\n",
              "      <td>destroy</td>\n",
              "      <td>5.90</td>\n",
              "      <td>v</td>\n",
              "      <td>v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>fresh</td>\n",
              "      <td>wide</td>\n",
              "      <td>0.40</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word1    word2  SimLex999 POS1 POS2\n",
              "934  reduce   shrink       8.02    v    v\n",
              "787    kill  destroy       5.90    v    v\n",
              "107   fresh     wide       0.40    a    a"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtB2CDOoUdR-",
        "colab_type": "text"
      },
      "source": [
        "## **Universal sentence encoder** \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FYtVC-yUZM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip3 install --quiet seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihSV9yc0VrzO",
        "colab_type": "code",
        "outputId": "45876d17-2b89-4588-db9f-a6e9bc75fa29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7BrHp6RVsV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxVtdHu0Vsff",
        "colab_type": "code",
        "outputId": "8a180a28-e174-472f-bb8a-26e70441da3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)\n",
        "\n",
        "# Compute a representation for each message, showing various lengths supported.\n",
        "word = \"breakfast\"\n",
        "messages = [word]\n",
        "\n",
        "# Reduce logging output.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  message_embeddings = session.run(embed(messages))\n",
        "\n",
        "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "    print(\"Message: {}\".format(messages[i]))\n",
        "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "    message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message: breakfast\n",
            "Embedding size: 512\n",
            "Embedding: [-0.016404513269662857, -0.04483848437666893, -0.0037806909531354904, ...]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GACecT2bFLXR",
        "colab_type": "text"
      },
      "source": [
        "# CBOW or word2index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm3z0IUzaQem",
        "colab_type": "text"
      },
      "source": [
        "Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paTtI75sX-uP",
        "colab_type": "code",
        "outputId": "64812e11-a073-49cf-b5b8-a169e634d173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment-network/reviews.txt\"\n",
        "#!cat reviews.txt | head"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-27 12:53:29--  https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment-network/reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33678267 (32M) [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]  32.12M  41.2MB/s    in 0.8s    \n",
            "\n",
            "2020-01-27 12:53:33 (41.2 MB/s) - ‘reviews.txt’ saved [33678267/33678267]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM0i-x2KZGCh",
        "colab_type": "code",
        "outputId": "7c88c600-160c-484a-9437-ba24d8a2ddbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment-network/labels.txt\"\n",
        "#!cat labels.txt | head -15"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-27 12:53:34--  https://raw.githubusercontent.com/udacity/deep-learning/master/sentiment-network/labels.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225000 (220K) [text/plain]\n",
            "Saving to: ‘labels.txt’\n",
            "\n",
            "labels.txt          100%[===================>] 219.73K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-01-27 12:53:36 (4.31 MB/s) - ‘labels.txt’ saved [225000/225000]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGAz0O3cb5sZ",
        "colab_type": "text"
      },
      "source": [
        "Predicting Movie Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1dkgoPdcDnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "f = open('reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "f = open('labels.txt')\n",
        "raw_labels = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
        "\n",
        "vocab = set()\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        if(len(word)>0):\n",
        "            vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "\n",
        "input_dataset = list()\n",
        "for sent in tokens:\n",
        "    sent_indices = list()\n",
        "    for word in sent:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(list(set(sent_indices)))\n",
        "\n",
        "target_dataset = list()\n",
        "for label in raw_labels:\n",
        "    if label == 'positive\\n':\n",
        "        target_dataset.append(1)\n",
        "    else:\n",
        "        target_dataset.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQytKPCD7pOI",
        "colab_type": "text"
      },
      "source": [
        "Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acKCboVMdVES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "alpha, iterations = (0.01, 10)\n",
        "hidden_size = 200\n",
        "\n",
        "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1\n",
        "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1\n",
        "\n",
        "\n",
        "#print (weights_0_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezMWtyPN7xIL",
        "colab_type": "text"
      },
      "source": [
        "Run the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0K0gwx8djmm",
        "colab_type": "code",
        "outputId": "a80488cd-6b62-48c4-a56a-34f76916383d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "\n",
        "correct,total = (0,0)\n",
        "\n",
        "for iter in range(iterations):\n",
        "    \n",
        "    # train on first 24,000\n",
        "    for i in range(len(input_dataset)-1000):\n",
        "\n",
        "        x,y = (input_dataset[i],target_dataset[i])\n",
        "        layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) #embed + sigmoid\n",
        "        layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) # linear + softmax\n",
        "\n",
        "        layer_2_delta = layer_2 - y # compare pred with truth\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) #backprop\n",
        "\n",
        "        weights_0_1[x] -= layer_1_delta * alpha\n",
        "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
        "\n",
        "        if(np.abs(layer_2_delta) < 0.5):\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        if(i % 10 == 9):\n",
        "            progress = str(i/float(len(input_dataset)))\n",
        "            sys.stdout.write('\\rIter:'+str(iter)\\\n",
        "                             +' Progress:'+progress[2:4]\\\n",
        "                             +'.'+progress[4:6]\\\n",
        "                             +'% Training Accuracy:'\\\n",
        "                             + str(correct/float(total)) + '%')\n",
        "    print()\n",
        "    \n",
        "correct,total = (0,0)\n",
        "for i in range(len(input_dataset)-1000,len(input_dataset)):\n",
        "\n",
        "    x = input_dataset[i]\n",
        "    y = target_dataset[i]\n",
        "\n",
        "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
        "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
        "    \n",
        "    if(np.abs(layer_2 - y) < 0.5):\n",
        "        correct += 1\n",
        "    total += 1\n",
        "print(\"Test Accuracy:\" + str(correct / float(total)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter:0 Progress:95.99% Training Accuracy:0.8309583333333334%\n",
            "Iter:1 Progress:95.99% Training Accuracy:0.8658541666666667%\n",
            "Iter:2 Progress:95.99% Training Accuracy:0.8840833333333333%\n",
            "Iter:3 Progress:95.99% Training Accuracy:0.89725%\n",
            "Iter:4 Progress:95.99% Training Accuracy:0.9074833333333333%\n",
            "Iter:5 Progress:95.99% Training Accuracy:0.9159861111111111%\n",
            "Iter:6 Progress:95.99% Training Accuracy:0.9231369047619048%\n",
            "Iter:7 Progress:95.99% Training Accuracy:0.929375%\n",
            "Iter:8 Progress:95.99% Training Accuracy:0.9350462962962963%\n",
            "Iter:9 Progress:95.99% Training Accuracy:0.9401%\n",
            "Test Accuracy:0.831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXY5GDA2keEM",
        "colab_type": "text"
      },
      "source": [
        "Let's get the vector for a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNxU2-seiKgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = \"dentist\"\n",
        "target_index = word2index[target]\n",
        "#weights_0_1[target_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqA3dw9lFZ5F",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ8Injlqjj09",
        "colab_type": "code",
        "outputId": "b0c99508-9f4b-405a-d39f-7aa0399e7cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip -O text8.zip"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-27 12:55:04--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M   344KB/s    in 93s     \n",
            "\n",
            "2020-01-27 12:56:38 (328 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRhXck_HmuGu",
        "colab_type": "code",
        "outputId": "a2a09296-76bb-405f-915a-c2b911c2cd4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip -o text8.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTFvUlmCmyG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --quiet gensim  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJemHGPSmyUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve-NjWtJm30H",
        "colab_type": "code",
        "outputId": "c05c709f-1a1c-4765-f8e4-462bd0198473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# https://github.com/dav/word2vec/blob/master/scripts/create-text8-vector-data.sh#L16\n",
        "model = word2vec.Word2Vec(corpus_file='text8',size=100, window=5, min_count=1, workers=10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOQCbfEeHXGO",
        "colab_type": "text"
      },
      "source": [
        "# Get the similarity scores of the three embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erpB0wQp_f4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjd_nn6UoPID",
        "colab_type": "code",
        "outputId": "d967ced9-a979-4973-d486-33cd30615c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "for num in randints:\n",
        "  word1 = simlex_rand.loc[simlex_rand.index == num]['word1'].values[0]\n",
        "  word2 = simlex_rand.loc[simlex_rand.index == num]['word2'].values[0]\n",
        "  print('Embedding this pair from Simlex_999: \"{}\" and \"{}\"'.format(word1, word2))\n",
        "  print('Running universal sentence encoder on \"{}\"'.format(word1))\n",
        "  messages = [word1]\n",
        "  # Reduce logging output.\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(messages))\n",
        "\n",
        "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "      #print(\"Message: {}\".format(messages[i]))\n",
        "      #print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "      message_embedding_snippet = \", \".join(\n",
        "          (str(x) for x in message_embedding[:3]))\n",
        "      #print(\"Embedding: [{}, ...]\".format(message_embedding_snippet))\n",
        "\n",
        "  vector1 = message_embedding\n",
        "  vector1 = np.asarray(vector1)\n",
        "  vector1 = vector1.reshape(1,512)\n",
        "\n",
        "  print('Running universal sentence encoder on \"{}\"'.format(word2))\n",
        "  messages = [word2]\n",
        "\n",
        "  # Reduce logging output.\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(messages))\n",
        "\n",
        "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "    #print(\"Message: {}\".format(messages[i]))\n",
        "    #print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "    message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "\n",
        "  vector2 = message_embedding\n",
        "  vector2 = np.asarray(vector2)\n",
        "  vector2 = vector2.reshape(1,512)\n",
        "\n",
        "  cos_sim = cosine_similarity(vector1, vector2)\n",
        "  print('Cosine similarity from Universal sentence encoder for {} and {} = {}\\n'.format(word1,word2,cos_sim) )\n",
        "  simlex_rand.loc[simlex_rand.index == num,'USE Cosine Sim'] = cos_sim"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding this pair from Simlex_999: \"reduce\" and \"shrink\"\n",
            "Running universal sentence encoder on \"reduce\"\n",
            "Running universal sentence encoder on \"shrink\"\n",
            "Cosine similarity from Universal sentence encoder for reduce and shrink = [[0.59188971]]\n",
            "\n",
            "Embedding this pair from Simlex_999: \"kill\" and \"destroy\"\n",
            "Running universal sentence encoder on \"kill\"\n",
            "Running universal sentence encoder on \"destroy\"\n",
            "Cosine similarity from Universal sentence encoder for kill and destroy = [[0.74396529]]\n",
            "\n",
            "Embedding this pair from Simlex_999: \"fresh\" and \"wide\"\n",
            "Running universal sentence encoder on \"fresh\"\n",
            "Running universal sentence encoder on \"wide\"\n",
            "Cosine similarity from Universal sentence encoder for fresh and wide = [[0.45424075]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxkSjMhF5b-e",
        "colab_type": "text"
      },
      "source": [
        "Embedding method 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLebzLYM5iP9",
        "colab_type": "code",
        "outputId": "aefbaa0a-14aa-45fc-fef0-aa472e615ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "for num in randints:\n",
        "  word1 = simlex_rand.loc[simlex_rand.index == num]['word1'].values[0]\n",
        "  word2 = simlex_rand.loc[simlex_rand.index == num]['word2'].values[0]\n",
        "  print('running word2index on the pair {} and {}'.format(word1,word2))\n",
        "  target_index1 = word2index[word1]\n",
        "  target_index2 = word2index[word2]\n",
        "  vector1 = weights_0_1[target_index1]\n",
        "  vector1 = vector1.reshape(1,200)\n",
        "  vector2 = weights_0_1[target_index2]\n",
        "  vector2 = vector2.reshape(1,200)\n",
        "  cos_sim = cosine_similarity(vector1, vector2)\n",
        "  print('Cosine similarity from word2index for {} and {} = {}\\n'.format(word1,word2,cos_sim))\n",
        "  simlex_rand.loc[simlex_rand.index == num,'word2index Cosine Sim'] = cos_sim\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running word2index on the pair reduce and shrink\n",
            "Cosine similarity from word2index for reduce and shrink = [[0.0400988]]\n",
            "\n",
            "running word2index on the pair kill and destroy\n",
            "Cosine similarity from word2index for kill and destroy = [[0.45333789]]\n",
            "\n",
            "running word2index on the pair fresh and wide\n",
            "Cosine similarity from word2index for fresh and wide = [[-0.09060798]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjkSQ3ob-wwF",
        "colab_type": "text"
      },
      "source": [
        "Embedding method word2vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1d--ZVE-qP5",
        "colab_type": "code",
        "outputId": "6276c736-02f4-4c8c-90ef-467785381d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "for num in randints:\n",
        "  word1 = simlex_rand.loc[simlex_rand.index == num]['word1'].values[0]\n",
        "  word2 = simlex_rand.loc[simlex_rand.index == num]['word2'].values[0]\n",
        "  print('running word2vec on the pair {} and {}'.format(word1,word2))\n",
        "  vector1 = model.wv[word1]\n",
        "  vector1 = vector1.reshape(1,100)\n",
        "  vector2 = model.wv[word2]\n",
        "  vector2 = vector2.reshape(1,100)\n",
        "  cos_sim = cosine_similarity(vector1, vector2)\n",
        "  print('Cosine similarity from word2vec for {} and {} = {}\\n'.format(word1,word2,cos_sim))\n",
        "  simlex_rand.loc[simlex_rand.index == num,'word2vec Cosine Sim'] = cos_sim\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running word2vec on the pair reduce and shrink\n",
            "Cosine similarity from word2vec for reduce and shrink = [[-0.18979317]]\n",
            "\n",
            "running word2vec on the pair kill and destroy\n",
            "Cosine similarity from word2vec for kill and destroy = [[0.9925319]]\n",
            "\n",
            "running word2vec on the pair fresh and wide\n",
            "Cosine similarity from word2vec for fresh and wide = [[0.9954476]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9hcrHk5E9jB",
        "colab_type": "code",
        "outputId": "9ce5bec8-c0a1-47b2-8af5-2abea5180f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "simlex_rand_out = simlex_rand[['word1','word2','SimLex999', 'USE Cosine Sim',\n",
        "       'word2index Cosine Sim', 'word2vec Cosine Sim']]\n",
        "simlex_rand_out.columns = ['word1','word2','SimLex999', 'USE',\n",
        "       'CBOW', 'word2vec']\n",
        "simlex_rand_out = round(simlex_rand_out,4)\n",
        "print('The cosine similarities from the 3 word embedding methods can be seen below.\\nThe similarity scores are not close to the Simlex999 scores\\n word2vec probably does best.')\n",
        "print(simlex_rand_out)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cosine similarities from the 3 word embedding methods can be seen below.\n",
            "The similarity scores are not close to the Simlex999 scores\n",
            " word2vec probably does best.\n",
            "      word1    word2  SimLex999     USE    CBOW  word2vec\n",
            "934  reduce   shrink       8.02  0.5919  0.0401   -0.1898\n",
            "787    kill  destroy       5.90  0.7440  0.4533    0.9925\n",
            "107   fresh     wide       0.40  0.4542 -0.0906    0.9954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pERP4JVjtPHm",
        "colab_type": "text"
      },
      "source": [
        "# Part 2, generate three .csv files with similarity scores of all words ending in 4, using my three embedded models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gON6qZ6yMC7D",
        "colab_type": "code",
        "outputId": "1548b3ef-a011-4b43-c8b4-9bfcf332adc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "## Unique id is jwb74, but python starts on zero, so I need to minus 1 to get the right words\n",
        "unique_id = 4 \n",
        "index = np.arange(4-1,1000,10)\n",
        "simlex_out = simlex.iloc[index,:].reset_index(drop=True)[['word1','word2','SimLex999']]\n",
        "simlex_out.columns = ['word1','word2','Sim_lex_Similarity']\n",
        "simlex_out"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>Sim_lex_Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>happy</td>\n",
              "      <td>cheerful</td>\n",
              "      <td>9.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bad</td>\n",
              "      <td>terrible</td>\n",
              "      <td>7.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dumb</td>\n",
              "      <td>foolish</td>\n",
              "      <td>6.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>weird</td>\n",
              "      <td>normal</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nice</td>\n",
              "      <td>cruel</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>try</td>\n",
              "      <td>think</td>\n",
              "      <td>2.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>leave</td>\n",
              "      <td>appear</td>\n",
              "      <td>0.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>put</td>\n",
              "      <td>hang</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>say</td>\n",
              "      <td>verify</td>\n",
              "      <td>4.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>absorb</td>\n",
              "      <td>possess</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     word1     word2  Sim_lex_Similarity\n",
              "0    happy  cheerful                9.55\n",
              "1      bad  terrible                7.78\n",
              "2     dumb   foolish                6.67\n",
              "3    weird    normal                0.72\n",
              "4     nice     cruel                0.67\n",
              "..     ...       ...                 ...\n",
              "95     try     think                2.62\n",
              "96   leave    appear                0.97\n",
              "97     put      hang                3.00\n",
              "98     say    verify                4.90\n",
              "99  absorb   possess                5.00\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcMPKVJvb5Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def printbold(string):\n",
        "  class bold:\n",
        "    BOLD = '\\033[1m'\n",
        "    ENDC = '\\033[0m'\n",
        "  print(f\"{bold.BOLD}{string}{bold.ENDC}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MRjxOaAvg6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9vklLaX1dm9",
        "colab_type": "text"
      },
      "source": [
        "## Embedding method 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RP9sqSEwN8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c98e055-e561-4b27-f629-87b604a6efa2"
      },
      "source": [
        "simlex_out_CBOW = copy.deepcopy(simlex_out)\n",
        "for num in np.arange(0,100,1):\n",
        "  printbold('word pair {}'.format(num))\n",
        "  word1 = simlex_out.loc[simlex_out.index == num]['word1'].values[0]\n",
        "  word2 = simlex_out.loc[simlex_out.index == num]['word2'].values[0]\n",
        "  print('running word2index on the pair {} and {}'.format(word1,word2))\n",
        "  try:\n",
        "    target_index1 = word2index[word1]\n",
        "    target_index2 = word2index[word2]\n",
        "    vector1 = weights_0_1[target_index1]\n",
        "    vector1 = vector1.reshape(1,200)\n",
        "    vector2 = weights_0_1[target_index2]\n",
        "    vector2 = vector2.reshape(1,200)\n",
        "    cos_sim = cosine_similarity(vector1, vector2)\n",
        "    print('Cosine similarity from word2index for {} and {} = {}'.format(word1,word2,cos_sim))\n",
        "    simlex_out_CBOW.loc[simlex_out_CBOW.index == num,'your_Similarity'] = cos_sim\n",
        "  except:\n",
        "    print('skipping the pair {} and {}'.format(word1,word2))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mword pair 0\u001b[0m\n",
            "running word2index on the pair happy and cheerful\n",
            "Cosine similarity from word2index for happy and cheerful = [[-0.36416571]]\n",
            "\u001b[1mword pair 1\u001b[0m\n",
            "running word2index on the pair bad and terrible\n",
            "Cosine similarity from word2index for bad and terrible = [[0.85411501]]\n",
            "\u001b[1mword pair 2\u001b[0m\n",
            "running word2index on the pair dumb and foolish\n",
            "Cosine similarity from word2index for dumb and foolish = [[0.12147803]]\n",
            "\u001b[1mword pair 3\u001b[0m\n",
            "running word2index on the pair weird and normal\n",
            "Cosine similarity from word2index for weird and normal = [[0.33807444]]\n",
            "\u001b[1mword pair 4\u001b[0m\n",
            "running word2index on the pair nice and cruel\n",
            "Cosine similarity from word2index for nice and cruel = [[-0.16975908]]\n",
            "\u001b[1mword pair 5\u001b[0m\n",
            "running word2index on the pair bad and terrific\n",
            "Cosine similarity from word2index for bad and terrific = [[-0.69049711]]\n",
            "\u001b[1mword pair 6\u001b[0m\n",
            "running word2index on the pair harsh and cruel\n",
            "Cosine similarity from word2index for harsh and cruel = [[-0.17936353]]\n",
            "\u001b[1mword pair 7\u001b[0m\n",
            "running word2index on the pair bad and guilty\n",
            "Cosine similarity from word2index for bad and guilty = [[0.72630404]]\n",
            "\u001b[1mword pair 8\u001b[0m\n",
            "running word2index on the pair exotic and rare\n",
            "Cosine similarity from word2index for exotic and rare = [[-0.73033919]]\n",
            "\u001b[1mword pair 9\u001b[0m\n",
            "running word2index on the pair sick and crazy\n",
            "Cosine similarity from word2index for sick and crazy = [[-0.19072776]]\n",
            "\u001b[1mword pair 10\u001b[0m\n",
            "running word2index on the pair dumb and rare\n",
            "Cosine similarity from word2index for dumb and rare = [[-0.16079473]]\n",
            "\u001b[1mword pair 11\u001b[0m\n",
            "running word2index on the pair groom and bride\n",
            "Cosine similarity from word2index for groom and bride = [[-0.10522914]]\n",
            "\u001b[1mword pair 12\u001b[0m\n",
            "running word2index on the pair leg and arm\n",
            "Cosine similarity from word2index for leg and arm = [[-0.30921562]]\n",
            "\u001b[1mword pair 13\u001b[0m\n",
            "running word2index on the pair dog and cat\n",
            "Cosine similarity from word2index for dog and cat = [[-0.20954933]]\n",
            "\u001b[1mword pair 14\u001b[0m\n",
            "running word2index on the pair taxi and cab\n",
            "Cosine similarity from word2index for taxi and cab = [[-0.02896764]]\n",
            "\u001b[1mword pair 15\u001b[0m\n",
            "running word2index on the pair toe and finger\n",
            "Cosine similarity from word2index for toe and finger = [[0.02715108]]\n",
            "\u001b[1mword pair 16\u001b[0m\n",
            "running word2index on the pair way and manner\n",
            "Cosine similarity from word2index for way and manner = [[0.0842096]]\n",
            "\u001b[1mword pair 17\u001b[0m\n",
            "running word2index on the pair paper and cardboard\n",
            "Cosine similarity from word2index for paper and cardboard = [[0.77635753]]\n",
            "\u001b[1mword pair 18\u001b[0m\n",
            "running word2index on the pair hand and wrist\n",
            "Cosine similarity from word2index for hand and wrist = [[0.08898179]]\n",
            "\u001b[1mword pair 19\u001b[0m\n",
            "running word2index on the pair band and orchestra\n",
            "Cosine similarity from word2index for band and orchestra = [[0.03519848]]\n",
            "\u001b[1mword pair 20\u001b[0m\n",
            "running word2index on the pair glass and crystal\n",
            "Cosine similarity from word2index for glass and crystal = [[0.0761322]]\n",
            "\u001b[1mword pair 21\u001b[0m\n",
            "running word2index on the pair grass and blade\n",
            "Cosine similarity from word2index for grass and blade = [[0.48504628]]\n",
            "\u001b[1mword pair 22\u001b[0m\n",
            "running word2index on the pair bar and jail\n",
            "Cosine similarity from word2index for bar and jail = [[0.09690428]]\n",
            "\u001b[1mword pair 23\u001b[0m\n",
            "running word2index on the pair bed and blanket\n",
            "Cosine similarity from word2index for bed and blanket = [[0.18815698]]\n",
            "\u001b[1mword pair 24\u001b[0m\n",
            "running word2index on the pair molecule and atom\n",
            "skipping the pair molecule and atom\n",
            "\u001b[1mword pair 25\u001b[0m\n",
            "running word2index on the pair size and magnitude\n",
            "Cosine similarity from word2index for size and magnitude = [[-0.08807422]]\n",
            "\u001b[1mword pair 26\u001b[0m\n",
            "running word2index on the pair god and spirit\n",
            "Cosine similarity from word2index for god and spirit = [[0.28712361]]\n",
            "\u001b[1mword pair 27\u001b[0m\n",
            "running word2index on the pair stomach and waist\n",
            "Cosine similarity from word2index for stomach and waist = [[0.05641654]]\n",
            "\u001b[1mword pair 28\u001b[0m\n",
            "running word2index on the pair mink and fur\n",
            "Cosine similarity from word2index for mink and fur = [[-0.12674766]]\n",
            "\u001b[1mword pair 29\u001b[0m\n",
            "running word2index on the pair water and salt\n",
            "Cosine similarity from word2index for water and salt = [[0.22212884]]\n",
            "\u001b[1mword pair 30\u001b[0m\n",
            "running word2index on the pair heroine and hero\n",
            "Cosine similarity from word2index for heroine and hero = [[0.17006905]]\n",
            "\u001b[1mword pair 31\u001b[0m\n",
            "running word2index on the pair grass and moss\n",
            "Cosine similarity from word2index for grass and moss = [[0.13610988]]\n",
            "\u001b[1mword pair 32\u001b[0m\n",
            "running word2index on the pair meat and bacon\n",
            "Cosine similarity from word2index for meat and bacon = [[-0.03917896]]\n",
            "\u001b[1mword pair 33\u001b[0m\n",
            "running word2index on the pair game and fun\n",
            "Cosine similarity from word2index for game and fun = [[0.59598469]]\n",
            "\u001b[1mword pair 34\u001b[0m\n",
            "running word2index on the pair crime and fraud\n",
            "Cosine similarity from word2index for crime and fraud = [[-0.44002338]]\n",
            "\u001b[1mword pair 35\u001b[0m\n",
            "running word2index on the pair wealth and poverty\n",
            "Cosine similarity from word2index for wealth and poverty = [[-0.02443567]]\n",
            "\u001b[1mword pair 36\u001b[0m\n",
            "running word2index on the pair oak and maple\n",
            "Cosine similarity from word2index for oak and maple = [[0.07721408]]\n",
            "\u001b[1mword pair 37\u001b[0m\n",
            "running word2index on the pair sun and sky\n",
            "Cosine similarity from word2index for sun and sky = [[0.16569313]]\n",
            "\u001b[1mword pair 38\u001b[0m\n",
            "running word2index on the pair arm and vein\n",
            "Cosine similarity from word2index for arm and vein = [[0.01069174]]\n",
            "\u001b[1mword pair 39\u001b[0m\n",
            "running word2index on the pair bee and ant\n",
            "Cosine similarity from word2index for bee and ant = [[-0.19949377]]\n",
            "\u001b[1mword pair 40\u001b[0m\n",
            "running word2index on the pair jar and bottle\n",
            "Cosine similarity from word2index for jar and bottle = [[-0.12481842]]\n",
            "\u001b[1mword pair 41\u001b[0m\n",
            "running word2index on the pair meter and yard\n",
            "Cosine similarity from word2index for meter and yard = [[0.15465288]]\n",
            "\u001b[1mword pair 42\u001b[0m\n",
            "running word2index on the pair bread and cheese\n",
            "Cosine similarity from word2index for bread and cheese = [[0.13406121]]\n",
            "\u001b[1mword pair 43\u001b[0m\n",
            "running word2index on the pair buddy and companion\n",
            "Cosine similarity from word2index for buddy and companion = [[0.06348339]]\n",
            "\u001b[1mword pair 44\u001b[0m\n",
            "running word2index on the pair death and catastrophe\n",
            "Cosine similarity from word2index for death and catastrophe = [[-0.13008011]]\n",
            "\u001b[1mword pair 45\u001b[0m\n",
            "running word2index on the pair butter and potato\n",
            "Cosine similarity from word2index for butter and potato = [[0.05717665]]\n",
            "\u001b[1mword pair 46\u001b[0m\n",
            "running word2index on the pair father and brother\n",
            "Cosine similarity from word2index for father and brother = [[0.13760856]]\n",
            "\u001b[1mword pair 47\u001b[0m\n",
            "running word2index on the pair cat and rabbit\n",
            "Cosine similarity from word2index for cat and rabbit = [[-0.50905639]]\n",
            "\u001b[1mword pair 48\u001b[0m\n",
            "running word2index on the pair winner and candidate\n",
            "Cosine similarity from word2index for winner and candidate = [[-0.19133198]]\n",
            "\u001b[1mword pair 49\u001b[0m\n",
            "running word2index on the pair floor and deck\n",
            "Cosine similarity from word2index for floor and deck = [[-0.14885412]]\n",
            "\u001b[1mword pair 50\u001b[0m\n",
            "running word2index on the pair communication and conversation\n",
            "Cosine similarity from word2index for communication and conversation = [[0.61545882]]\n",
            "\u001b[1mword pair 51\u001b[0m\n",
            "running word2index on the pair college and class\n",
            "Cosine similarity from word2index for college and class = [[0.06744525]]\n",
            "\u001b[1mword pair 52\u001b[0m\n",
            "running word2index on the pair whiskey and champagne\n",
            "Cosine similarity from word2index for whiskey and champagne = [[-0.02505705]]\n",
            "\u001b[1mword pair 53\u001b[0m\n",
            "running word2index on the pair head and nail\n",
            "Cosine similarity from word2index for head and nail = [[0.07487752]]\n",
            "\u001b[1mword pair 54\u001b[0m\n",
            "running word2index on the pair reflection and image\n",
            "Cosine similarity from word2index for reflection and image = [[0.00457731]]\n",
            "\u001b[1mword pair 55\u001b[0m\n",
            "running word2index on the pair cab and bus\n",
            "Cosine similarity from word2index for cab and bus = [[-0.02412424]]\n",
            "\u001b[1mword pair 56\u001b[0m\n",
            "running word2index on the pair actor and singer\n",
            "Cosine similarity from word2index for actor and singer = [[0.31193397]]\n",
            "\u001b[1mword pair 57\u001b[0m\n",
            "running word2index on the pair mood and emotion\n",
            "Cosine similarity from word2index for mood and emotion = [[-0.07631369]]\n",
            "\u001b[1mword pair 58\u001b[0m\n",
            "running word2index on the pair comfort and safety\n",
            "Cosine similarity from word2index for comfort and safety = [[0.502882]]\n",
            "\u001b[1mword pair 59\u001b[0m\n",
            "running word2index on the pair activity and movement\n",
            "Cosine similarity from word2index for activity and movement = [[0.14410205]]\n",
            "\u001b[1mword pair 60\u001b[0m\n",
            "running word2index on the pair adult and guardian\n",
            "Cosine similarity from word2index for adult and guardian = [[-0.08116354]]\n",
            "\u001b[1mword pair 61\u001b[0m\n",
            "running word2index on the pair nurse and scientist\n",
            "Cosine similarity from word2index for nurse and scientist = [[0.4564511]]\n",
            "\u001b[1mword pair 62\u001b[0m\n",
            "running word2index on the pair hip and lip\n",
            "Cosine similarity from word2index for hip and lip = [[0.02422427]]\n",
            "\u001b[1mword pair 63\u001b[0m\n",
            "running word2index on the pair bed and couch\n",
            "Cosine similarity from word2index for bed and couch = [[0.09356158]]\n",
            "\u001b[1mword pair 64\u001b[0m\n",
            "running word2index on the pair pipe and cigar\n",
            "Cosine similarity from word2index for pipe and cigar = [[-0.00989624]]\n",
            "\u001b[1mword pair 65\u001b[0m\n",
            "running word2index on the pair cup and jar\n",
            "Cosine similarity from word2index for cup and jar = [[-0.15264604]]\n",
            "\u001b[1mword pair 66\u001b[0m\n",
            "running word2index on the pair steeple and chapel\n",
            "Cosine similarity from word2index for steeple and chapel = [[-0.26362591]]\n",
            "\u001b[1mword pair 67\u001b[0m\n",
            "running word2index on the pair bottle and container\n",
            "Cosine similarity from word2index for bottle and container = [[0.02617817]]\n",
            "\u001b[1mword pair 68\u001b[0m\n",
            "running word2index on the pair calf and bull\n",
            "Cosine similarity from word2index for calf and bull = [[-0.1140436]]\n",
            "\u001b[1mword pair 69\u001b[0m\n",
            "running word2index on the pair dinner and chicken\n",
            "Cosine similarity from word2index for dinner and chicken = [[-0.48435092]]\n",
            "\u001b[1mword pair 70\u001b[0m\n",
            "running word2index on the pair attention and awareness\n",
            "Cosine similarity from word2index for attention and awareness = [[0.46623021]]\n",
            "\u001b[1mword pair 71\u001b[0m\n",
            "running word2index on the pair arithmetic and rhythm\n",
            "Cosine similarity from word2index for arithmetic and rhythm = [[-0.07028395]]\n",
            "\u001b[1mword pair 72\u001b[0m\n",
            "running word2index on the pair flower and endurance\n",
            "Cosine similarity from word2index for flower and endurance = [[-0.40290314]]\n",
            "\u001b[1mword pair 73\u001b[0m\n",
            "running word2index on the pair liquor and century\n",
            "Cosine similarity from word2index for liquor and century = [[-0.42249968]]\n",
            "\u001b[1mword pair 74\u001b[0m\n",
            "running word2index on the pair course and stomach\n",
            "Cosine similarity from word2index for course and stomach = [[-0.43252789]]\n",
            "\u001b[1mword pair 75\u001b[0m\n",
            "running word2index on the pair apple and sunshine\n",
            "Cosine similarity from word2index for apple and sunshine = [[0.0582311]]\n",
            "\u001b[1mword pair 76\u001b[0m\n",
            "running word2index on the pair rice and boy\n",
            "Cosine similarity from word2index for rice and boy = [[-0.37930821]]\n",
            "\u001b[1mword pair 77\u001b[0m\n",
            "running word2index on the pair stomach and bedroom\n",
            "Cosine similarity from word2index for stomach and bedroom = [[0.30731415]]\n",
            "\u001b[1mword pair 78\u001b[0m\n",
            "running word2index on the pair multiply and divide\n",
            "Cosine similarity from word2index for multiply and divide = [[0.13299523]]\n",
            "\u001b[1mword pair 79\u001b[0m\n",
            "running word2index on the pair choose and elect\n",
            "Cosine similarity from word2index for choose and elect = [[0.00611605]]\n",
            "\u001b[1mword pair 80\u001b[0m\n",
            "running word2index on the pair advise and recommend\n",
            "Cosine similarity from word2index for advise and recommend = [[-0.14981129]]\n",
            "\u001b[1mword pair 81\u001b[0m\n",
            "running word2index on the pair understand and know\n",
            "Cosine similarity from word2index for understand and know = [[-0.0215955]]\n",
            "\u001b[1mword pair 82\u001b[0m\n",
            "running word2index on the pair come and attend\n",
            "Cosine similarity from word2index for come and attend = [[0.00395525]]\n",
            "\u001b[1mword pair 83\u001b[0m\n",
            "running word2index on the pair arrive and come\n",
            "Cosine similarity from word2index for arrive and come = [[0.15703408]]\n",
            "\u001b[1mword pair 84\u001b[0m\n",
            "running word2index on the pair take and carry\n",
            "Cosine similarity from word2index for take and carry = [[0.24247392]]\n",
            "\u001b[1mword pair 85\u001b[0m\n",
            "running word2index on the pair lose and keep\n",
            "Cosine similarity from word2index for lose and keep = [[0.20884437]]\n",
            "\u001b[1mword pair 86\u001b[0m\n",
            "running word2index on the pair forget and know\n",
            "Cosine similarity from word2index for forget and know = [[0.12539744]]\n",
            "\u001b[1mword pair 87\u001b[0m\n",
            "running word2index on the pair pretend and imagine\n",
            "Cosine similarity from word2index for pretend and imagine = [[0.02857772]]\n",
            "\u001b[1mword pair 88\u001b[0m\n",
            "running word2index on the pair say and participate\n",
            "Cosine similarity from word2index for say and participate = [[0.31605126]]\n",
            "\u001b[1mword pair 89\u001b[0m\n",
            "running word2index on the pair ask and plead\n",
            "Cosine similarity from word2index for ask and plead = [[-0.36895647]]\n",
            "\u001b[1mword pair 90\u001b[0m\n",
            "running word2index on the pair give and put\n",
            "Cosine similarity from word2index for give and put = [[0.07891764]]\n",
            "\u001b[1mword pair 91\u001b[0m\n",
            "running word2index on the pair understand and forgive\n",
            "Cosine similarity from word2index for understand and forgive = [[0.0348104]]\n",
            "\u001b[1mword pair 92\u001b[0m\n",
            "running word2index on the pair accept and forgive\n",
            "Cosine similarity from word2index for accept and forgive = [[0.12106107]]\n",
            "\u001b[1mword pair 93\u001b[0m\n",
            "running word2index on the pair arrange and organize\n",
            "Cosine similarity from word2index for arrange and organize = [[0.09294432]]\n",
            "\u001b[1mword pair 94\u001b[0m\n",
            "running word2index on the pair pretend and seem\n",
            "Cosine similarity from word2index for pretend and seem = [[-0.07134768]]\n",
            "\u001b[1mword pair 95\u001b[0m\n",
            "running word2index on the pair try and think\n",
            "Cosine similarity from word2index for try and think = [[-0.19865365]]\n",
            "\u001b[1mword pair 96\u001b[0m\n",
            "running word2index on the pair leave and appear\n",
            "Cosine similarity from word2index for leave and appear = [[0.02145442]]\n",
            "\u001b[1mword pair 97\u001b[0m\n",
            "running word2index on the pair put and hang\n",
            "Cosine similarity from word2index for put and hang = [[-0.0251675]]\n",
            "\u001b[1mword pair 98\u001b[0m\n",
            "running word2index on the pair say and verify\n",
            "Cosine similarity from word2index for say and verify = [[0.05733741]]\n",
            "\u001b[1mword pair 99\u001b[0m\n",
            "running word2index on the pair absorb and possess\n",
            "Cosine similarity from word2index for absorb and possess = [[0.44126203]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3H9OAl606VI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "47b87835-e6fe-4613-db82-0837727b4556"
      },
      "source": [
        "simlex_out_CBOW.to_csv('CBOW.csv',sep='\\t')\n",
        "files.download('CBOW.csv')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-11447ee39230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msimlex_out_CBOW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CBOW.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CBOW.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_Rj2d0XQKb",
        "colab_type": "text"
      },
      "source": [
        "## Embedding method 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWsrUWzN112f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simlex_out_word2vec = copy.deepcopy(simlex_out)\n",
        "for num in np.arange(0,100,1):\n",
        "  printbold('word pair {}'.format(num))\n",
        "  word1 = simlex_out.loc[simlex_out.index == num]['word1'].values[0]\n",
        "  word2 = simlex_out.loc[simlex_out.index == num]['word2'].values[0]\n",
        "  try:\n",
        "    print('running word2vec on the pair {} and {}'.format(word1,word2))\n",
        "    vector1 = model.wv[word1]\n",
        "    vector1 = vector1.reshape(1,100)\n",
        "    vector2 = model.wv[word2]\n",
        "    vector2 = vector2.reshape(1,100)\n",
        "    cos_sim = cosine_similarity(vector1, vector2)\n",
        "    print('Cosine similarity from word2vec for {} and {} = {}'.format(word1,word2,cos_sim))\n",
        "    simlex_out_word2vec.loc[simlex_out_word2vec.index == num,'your_Similarity'] = cos_sim\n",
        "  except:\n",
        "    print('skipping the pair {} and {}'.format(word1,word2))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9SElR5w1I2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simlex_out_word2vec.to_csv('word2vec.csv',sep='\\t')\n",
        "files.download('word2vec.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEZ3RCjmzPIr",
        "colab_type": "text"
      },
      "source": [
        "## Embedding method 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ydYyapu93I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6284d68d-82b9-40ea-d465-086764b45db7"
      },
      "source": [
        "\n",
        "simlex_out_USE = copy.deepcopy(simlex_out)\n",
        "for num in np.arange(0,50,1):\n",
        "  printbold('word pair {}'.format(num))\n",
        "  word1 = simlex_out.loc[simlex_out.index == num]['word1'].values[0]\n",
        "  word2 = simlex_out.loc[simlex_out.index == num]['word2'].values[0]\n",
        "  try:\n",
        "    print('Embedding this pair from Simlex_999: \"{}\" and \"{}\"'.format(word1, word2))\n",
        "    print('Running universal sentence encoder on \"{}\"'.format(word1))\n",
        "    messages = [word1]\n",
        "    # Reduce logging output.\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(messages))\n",
        "\n",
        "      for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "        #print(\"Message: {}\".format(messages[i]))\n",
        "        #print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "        message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "\n",
        "    vector1 = message_embedding\n",
        "    vector1 = np.asarray(vector1)\n",
        "    vector1 = vector1.reshape(1,512)\n",
        "\n",
        "    print('Running universal sentence encoder on \"{}\"'.format(word2))\n",
        "    messages = [word2]\n",
        "\n",
        "    # Reduce logging output.\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(messages))\n",
        "      \n",
        "      for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "        #print(\"Message: {}\".format(messages[i]))\n",
        "        #print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "        message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "\n",
        "    vector2 = message_embedding\n",
        "    vector2 = np.asarray(vector2)\n",
        "    vector2 = vector2.reshape(1,512)\n",
        "\n",
        "    cos_sim = cosine_similarity(vector1, vector2)\n",
        "    print('Cosine similarity from Universal sentence encoder for {} and {} = {}\\n'.format(word1,word2,cos_sim) )\n",
        "    simlex_out_USE.loc[simlex_out_USE.index == num,'your_Similarity'] = cos_sim\n",
        "  except:\n",
        "    print('skipping the pair {} and {}'.format(word1,word2))\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mword pair 0\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"happy\" and \"cheerful\"\n",
            "Running universal sentence encoder on \"happy\"\n",
            "Running universal sentence encoder on \"cheerful\"\n",
            "Cosine similarity from Universal sentence encoder for happy and cheerful = [[0.78390019]]\n",
            "\n",
            "\u001b[1mword pair 1\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bad\" and \"terrible\"\n",
            "Running universal sentence encoder on \"bad\"\n",
            "Running universal sentence encoder on \"terrible\"\n",
            "Cosine similarity from Universal sentence encoder for bad and terrible = [[0.86002109]]\n",
            "\n",
            "\u001b[1mword pair 2\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"dumb\" and \"foolish\"\n",
            "Running universal sentence encoder on \"dumb\"\n",
            "Running universal sentence encoder on \"foolish\"\n",
            "Cosine similarity from Universal sentence encoder for dumb and foolish = [[0.70472723]]\n",
            "\n",
            "\u001b[1mword pair 3\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"weird\" and \"normal\"\n",
            "Running universal sentence encoder on \"weird\"\n",
            "Running universal sentence encoder on \"normal\"\n",
            "Cosine similarity from Universal sentence encoder for weird and normal = [[0.59703013]]\n",
            "\n",
            "\u001b[1mword pair 4\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"nice\" and \"cruel\"\n",
            "Running universal sentence encoder on \"nice\"\n",
            "Running universal sentence encoder on \"cruel\"\n",
            "Cosine similarity from Universal sentence encoder for nice and cruel = [[0.48296259]]\n",
            "\n",
            "\u001b[1mword pair 5\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bad\" and \"terrific\"\n",
            "Running universal sentence encoder on \"bad\"\n",
            "Running universal sentence encoder on \"terrific\"\n",
            "Cosine similarity from Universal sentence encoder for bad and terrific = [[0.60270929]]\n",
            "\n",
            "\u001b[1mword pair 6\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"harsh\" and \"cruel\"\n",
            "Running universal sentence encoder on \"harsh\"\n",
            "Running universal sentence encoder on \"cruel\"\n",
            "Cosine similarity from Universal sentence encoder for harsh and cruel = [[0.74008342]]\n",
            "\n",
            "\u001b[1mword pair 7\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bad\" and \"guilty\"\n",
            "Running universal sentence encoder on \"bad\"\n",
            "Running universal sentence encoder on \"guilty\"\n",
            "Cosine similarity from Universal sentence encoder for bad and guilty = [[0.39791468]]\n",
            "\n",
            "\u001b[1mword pair 8\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"exotic\" and \"rare\"\n",
            "Running universal sentence encoder on \"exotic\"\n",
            "Running universal sentence encoder on \"rare\"\n",
            "Cosine similarity from Universal sentence encoder for exotic and rare = [[0.52756067]]\n",
            "\n",
            "\u001b[1mword pair 9\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"sick\" and \"crazy\"\n",
            "Running universal sentence encoder on \"sick\"\n",
            "Running universal sentence encoder on \"crazy\"\n",
            "Cosine similarity from Universal sentence encoder for sick and crazy = [[0.56260504]]\n",
            "\n",
            "\u001b[1mword pair 10\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"dumb\" and \"rare\"\n",
            "Running universal sentence encoder on \"dumb\"\n",
            "Running universal sentence encoder on \"rare\"\n",
            "Cosine similarity from Universal sentence encoder for dumb and rare = [[0.33797869]]\n",
            "\n",
            "\u001b[1mword pair 11\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"groom\" and \"bride\"\n",
            "Running universal sentence encoder on \"groom\"\n",
            "Running universal sentence encoder on \"bride\"\n",
            "Cosine similarity from Universal sentence encoder for groom and bride = [[0.7233687]]\n",
            "\n",
            "\u001b[1mword pair 12\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"leg\" and \"arm\"\n",
            "Running universal sentence encoder on \"leg\"\n",
            "Running universal sentence encoder on \"arm\"\n",
            "Cosine similarity from Universal sentence encoder for leg and arm = [[0.70986537]]\n",
            "\n",
            "\u001b[1mword pair 13\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"dog\" and \"cat\"\n",
            "Running universal sentence encoder on \"dog\"\n",
            "Running universal sentence encoder on \"cat\"\n",
            "Cosine similarity from Universal sentence encoder for dog and cat = [[0.71145336]]\n",
            "\n",
            "\u001b[1mword pair 14\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"taxi\" and \"cab\"\n",
            "Running universal sentence encoder on \"taxi\"\n",
            "Running universal sentence encoder on \"cab\"\n",
            "Cosine similarity from Universal sentence encoder for taxi and cab = [[0.81794404]]\n",
            "\n",
            "\u001b[1mword pair 15\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"toe\" and \"finger\"\n",
            "Running universal sentence encoder on \"toe\"\n",
            "Running universal sentence encoder on \"finger\"\n",
            "Cosine similarity from Universal sentence encoder for toe and finger = [[0.69266566]]\n",
            "\n",
            "\u001b[1mword pair 16\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"way\" and \"manner\"\n",
            "Running universal sentence encoder on \"way\"\n",
            "Running universal sentence encoder on \"manner\"\n",
            "Cosine similarity from Universal sentence encoder for way and manner = [[0.76237109]]\n",
            "\n",
            "\u001b[1mword pair 17\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"paper\" and \"cardboard\"\n",
            "Running universal sentence encoder on \"paper\"\n",
            "Running universal sentence encoder on \"cardboard\"\n",
            "Cosine similarity from Universal sentence encoder for paper and cardboard = [[0.68461406]]\n",
            "\n",
            "\u001b[1mword pair 18\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"hand\" and \"wrist\"\n",
            "Running universal sentence encoder on \"hand\"\n",
            "Running universal sentence encoder on \"wrist\"\n",
            "Cosine similarity from Universal sentence encoder for hand and wrist = [[0.65672611]]\n",
            "\n",
            "\u001b[1mword pair 19\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"band\" and \"orchestra\"\n",
            "Running universal sentence encoder on \"band\"\n",
            "Running universal sentence encoder on \"orchestra\"\n",
            "Cosine similarity from Universal sentence encoder for band and orchestra = [[0.7203786]]\n",
            "\n",
            "\u001b[1mword pair 20\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"glass\" and \"crystal\"\n",
            "Running universal sentence encoder on \"glass\"\n",
            "Running universal sentence encoder on \"crystal\"\n",
            "Cosine similarity from Universal sentence encoder for glass and crystal = [[0.59349916]]\n",
            "\n",
            "\u001b[1mword pair 21\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"grass\" and \"blade\"\n",
            "Running universal sentence encoder on \"grass\"\n",
            "Running universal sentence encoder on \"blade\"\n",
            "Cosine similarity from Universal sentence encoder for grass and blade = [[0.32042427]]\n",
            "\n",
            "\u001b[1mword pair 22\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bar\" and \"jail\"\n",
            "Running universal sentence encoder on \"bar\"\n",
            "Running universal sentence encoder on \"jail\"\n",
            "Cosine similarity from Universal sentence encoder for bar and jail = [[0.49545988]]\n",
            "\n",
            "\u001b[1mword pair 23\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bed\" and \"blanket\"\n",
            "Running universal sentence encoder on \"bed\"\n",
            "Running universal sentence encoder on \"blanket\"\n",
            "Cosine similarity from Universal sentence encoder for bed and blanket = [[0.67384246]]\n",
            "\n",
            "\u001b[1mword pair 24\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"molecule\" and \"atom\"\n",
            "Running universal sentence encoder on \"molecule\"\n",
            "Running universal sentence encoder on \"atom\"\n",
            "Cosine similarity from Universal sentence encoder for molecule and atom = [[0.70301]]\n",
            "\n",
            "\u001b[1mword pair 25\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"size\" and \"magnitude\"\n",
            "Running universal sentence encoder on \"size\"\n",
            "Running universal sentence encoder on \"magnitude\"\n",
            "Cosine similarity from Universal sentence encoder for size and magnitude = [[0.30020995]]\n",
            "\n",
            "\u001b[1mword pair 26\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"god\" and \"spirit\"\n",
            "Running universal sentence encoder on \"god\"\n",
            "Running universal sentence encoder on \"spirit\"\n",
            "Cosine similarity from Universal sentence encoder for god and spirit = [[0.6355489]]\n",
            "\n",
            "\u001b[1mword pair 27\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"stomach\" and \"waist\"\n",
            "Running universal sentence encoder on \"stomach\"\n",
            "Running universal sentence encoder on \"waist\"\n",
            "Cosine similarity from Universal sentence encoder for stomach and waist = [[0.69768899]]\n",
            "\n",
            "\u001b[1mword pair 28\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"mink\" and \"fur\"\n",
            "Running universal sentence encoder on \"mink\"\n",
            "Running universal sentence encoder on \"fur\"\n",
            "Cosine similarity from Universal sentence encoder for mink and fur = [[0.69724753]]\n",
            "\n",
            "\u001b[1mword pair 29\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"water\" and \"salt\"\n",
            "Running universal sentence encoder on \"water\"\n",
            "Running universal sentence encoder on \"salt\"\n",
            "Cosine similarity from Universal sentence encoder for water and salt = [[0.56756767]]\n",
            "\n",
            "\u001b[1mword pair 30\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"heroine\" and \"hero\"\n",
            "Running universal sentence encoder on \"heroine\"\n",
            "Running universal sentence encoder on \"hero\"\n",
            "Cosine similarity from Universal sentence encoder for heroine and hero = [[0.4244658]]\n",
            "\n",
            "\u001b[1mword pair 31\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"grass\" and \"moss\"\n",
            "Running universal sentence encoder on \"grass\"\n",
            "Running universal sentence encoder on \"moss\"\n",
            "Cosine similarity from Universal sentence encoder for grass and moss = [[0.59414938]]\n",
            "\n",
            "\u001b[1mword pair 32\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"meat\" and \"bacon\"\n",
            "Running universal sentence encoder on \"meat\"\n",
            "Running universal sentence encoder on \"bacon\"\n",
            "Cosine similarity from Universal sentence encoder for meat and bacon = [[0.70824624]]\n",
            "\n",
            "\u001b[1mword pair 33\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"game\" and \"fun\"\n",
            "Running universal sentence encoder on \"game\"\n",
            "Running universal sentence encoder on \"fun\"\n",
            "Cosine similarity from Universal sentence encoder for game and fun = [[0.57149597]]\n",
            "\n",
            "\u001b[1mword pair 34\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"crime\" and \"fraud\"\n",
            "Running universal sentence encoder on \"crime\"\n",
            "Running universal sentence encoder on \"fraud\"\n",
            "Cosine similarity from Universal sentence encoder for crime and fraud = [[0.53188673]]\n",
            "\n",
            "\u001b[1mword pair 35\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"wealth\" and \"poverty\"\n",
            "Running universal sentence encoder on \"wealth\"\n",
            "Running universal sentence encoder on \"poverty\"\n",
            "Cosine similarity from Universal sentence encoder for wealth and poverty = [[0.73509486]]\n",
            "\n",
            "\u001b[1mword pair 36\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"oak\" and \"maple\"\n",
            "Running universal sentence encoder on \"oak\"\n",
            "Running universal sentence encoder on \"maple\"\n",
            "Cosine similarity from Universal sentence encoder for oak and maple = [[0.69856698]]\n",
            "\n",
            "\u001b[1mword pair 37\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"sun\" and \"sky\"\n",
            "Running universal sentence encoder on \"sun\"\n",
            "Running universal sentence encoder on \"sky\"\n",
            "Cosine similarity from Universal sentence encoder for sun and sky = [[0.73268602]]\n",
            "\n",
            "\u001b[1mword pair 38\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"arm\" and \"vein\"\n",
            "Running universal sentence encoder on \"arm\"\n",
            "Running universal sentence encoder on \"vein\"\n",
            "Cosine similarity from Universal sentence encoder for arm and vein = [[0.61761567]]\n",
            "\n",
            "\u001b[1mword pair 39\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bee\" and \"ant\"\n",
            "Running universal sentence encoder on \"bee\"\n",
            "Running universal sentence encoder on \"ant\"\n",
            "Cosine similarity from Universal sentence encoder for bee and ant = [[0.70943134]]\n",
            "\n",
            "\u001b[1mword pair 40\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"jar\" and \"bottle\"\n",
            "Running universal sentence encoder on \"jar\"\n",
            "Running universal sentence encoder on \"bottle\"\n",
            "Cosine similarity from Universal sentence encoder for jar and bottle = [[0.63924085]]\n",
            "\n",
            "\u001b[1mword pair 41\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"meter\" and \"yard\"\n",
            "Running universal sentence encoder on \"meter\"\n",
            "Running universal sentence encoder on \"yard\"\n",
            "Cosine similarity from Universal sentence encoder for meter and yard = [[0.56107189]]\n",
            "\n",
            "\u001b[1mword pair 42\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bread\" and \"cheese\"\n",
            "Running universal sentence encoder on \"bread\"\n",
            "Running universal sentence encoder on \"cheese\"\n",
            "Cosine similarity from Universal sentence encoder for bread and cheese = [[0.73801536]]\n",
            "\n",
            "\u001b[1mword pair 43\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"buddy\" and \"companion\"\n",
            "Running universal sentence encoder on \"buddy\"\n",
            "Running universal sentence encoder on \"companion\"\n",
            "Cosine similarity from Universal sentence encoder for buddy and companion = [[0.52294879]]\n",
            "\n",
            "\u001b[1mword pair 44\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"death\" and \"catastrophe\"\n",
            "Running universal sentence encoder on \"death\"\n",
            "Running universal sentence encoder on \"catastrophe\"\n",
            "Cosine similarity from Universal sentence encoder for death and catastrophe = [[0.52598307]]\n",
            "\n",
            "\u001b[1mword pair 45\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"butter\" and \"potato\"\n",
            "Running universal sentence encoder on \"butter\"\n",
            "Running universal sentence encoder on \"potato\"\n",
            "Cosine similarity from Universal sentence encoder for butter and potato = [[0.59261596]]\n",
            "\n",
            "\u001b[1mword pair 46\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"father\" and \"brother\"\n",
            "Running universal sentence encoder on \"father\"\n",
            "Running universal sentence encoder on \"brother\"\n",
            "Cosine similarity from Universal sentence encoder for father and brother = [[0.72717228]]\n",
            "\n",
            "\u001b[1mword pair 47\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"cat\" and \"rabbit\"\n",
            "Running universal sentence encoder on \"cat\"\n",
            "Running universal sentence encoder on \"rabbit\"\n",
            "Cosine similarity from Universal sentence encoder for cat and rabbit = [[0.56566462]]\n",
            "\n",
            "\u001b[1mword pair 48\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"winner\" and \"candidate\"\n",
            "Running universal sentence encoder on \"winner\"\n",
            "Running universal sentence encoder on \"candidate\"\n",
            "Cosine similarity from Universal sentence encoder for winner and candidate = [[0.4948249]]\n",
            "\n",
            "\u001b[1mword pair 49\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"floor\" and \"deck\"\n",
            "Running universal sentence encoder on \"floor\"\n",
            "Running universal sentence encoder on \"deck\"\n",
            "Cosine similarity from Universal sentence encoder for floor and deck = [[0.51146346]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRerlgquFtt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simlex_out_USE_1 = copy.deepcopy(simlex_out_USE)\n",
        "simlex_out_USE_1 = simlex_out_USE_1.iloc[0:50,:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChYiFheHysEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simlex_out_USE_1.to_csv('universal_sentence_encoder_1.csv',sep='\\t')\n",
        "files.download('universal_sentence_encoder_1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y63jOvkr1Vt7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2dad7e2c-fed7-409e-e867-c89bef9e707c"
      },
      "source": [
        "\n",
        "simlex_out_USE = copy.deepcopy(simlex_out)\n",
        "for num in np.arange(50,100,1):\n",
        "  printbold('word pair {}'.format(num))\n",
        "  word1 = simlex_out.loc[simlex_out.index == num]['word1'].values[0]\n",
        "  word2 = simlex_out.loc[simlex_out.index == num]['word2'].values[0]\n",
        "  try:\n",
        "    print('Embedding this pair from Simlex_999: \"{}\" and \"{}\"'.format(word1, word2))\n",
        "    print('Running universal sentence encoder on \"{}\"'.format(word1))\n",
        "    messages = [word1]\n",
        "    # Reduce logging output.\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(messages))\n",
        "\n",
        "      for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "        #print(\"Message: {}\".format(messages[i]))\n",
        "        #print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "        message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "\n",
        "    vector1 = message_embedding\n",
        "    vector1 = np.asarray(vector1)\n",
        "    vector1 = vector1.reshape(1,512)\n",
        "\n",
        "    print('Running universal sentence encoder on \"{}\"'.format(word2))\n",
        "    messages = [word2]\n",
        "\n",
        "    # Reduce logging output.\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "    with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "      message_embeddings = session.run(embed(messages))\n",
        "      \n",
        "      for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "        #print(\"Message: {}\".format(messages[i]))\n",
        "        #print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "        message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "\n",
        "    vector2 = message_embedding\n",
        "    vector2 = np.asarray(vector2)\n",
        "    vector2 = vector2.reshape(1,512)\n",
        "\n",
        "    cos_sim = cosine_similarity(vector1, vector2)\n",
        "    print('Cosine similarity from Universal sentence encoder for {} and {} = {}\\n'.format(word1,word2,cos_sim) )\n",
        "    simlex_out_USE.loc[simlex_out_USE.index == num,'your_Similarity'] = cos_sim\n",
        "  except:\n",
        "    print('skipping the pair {} and {}'.format(word1,word2))\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mword pair 50\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"communication\" and \"conversation\"\n",
            "Running universal sentence encoder on \"communication\"\n",
            "Running universal sentence encoder on \"conversation\"\n",
            "Cosine similarity from Universal sentence encoder for communication and conversation = [[0.58857998]]\n",
            "\n",
            "\u001b[1mword pair 51\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"college\" and \"class\"\n",
            "Running universal sentence encoder on \"college\"\n",
            "Running universal sentence encoder on \"class\"\n",
            "Cosine similarity from Universal sentence encoder for college and class = [[0.54269196]]\n",
            "\n",
            "\u001b[1mword pair 52\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"whiskey\" and \"champagne\"\n",
            "Running universal sentence encoder on \"whiskey\"\n",
            "Running universal sentence encoder on \"champagne\"\n",
            "Cosine similarity from Universal sentence encoder for whiskey and champagne = [[0.68791156]]\n",
            "\n",
            "\u001b[1mword pair 53\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"head\" and \"nail\"\n",
            "Running universal sentence encoder on \"head\"\n",
            "Running universal sentence encoder on \"nail\"\n",
            "Cosine similarity from Universal sentence encoder for head and nail = [[0.64312791]]\n",
            "\n",
            "\u001b[1mword pair 54\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"reflection\" and \"image\"\n",
            "Running universal sentence encoder on \"reflection\"\n",
            "Running universal sentence encoder on \"image\"\n",
            "Cosine similarity from Universal sentence encoder for reflection and image = [[0.53453951]]\n",
            "\n",
            "\u001b[1mword pair 55\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"cab\" and \"bus\"\n",
            "Running universal sentence encoder on \"cab\"\n",
            "Running universal sentence encoder on \"bus\"\n",
            "Cosine similarity from Universal sentence encoder for cab and bus = [[0.68412284]]\n",
            "\n",
            "\u001b[1mword pair 56\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"actor\" and \"singer\"\n",
            "Running universal sentence encoder on \"actor\"\n",
            "Running universal sentence encoder on \"singer\"\n",
            "Cosine similarity from Universal sentence encoder for actor and singer = [[0.72196795]]\n",
            "\n",
            "\u001b[1mword pair 57\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"mood\" and \"emotion\"\n",
            "Running universal sentence encoder on \"mood\"\n",
            "Running universal sentence encoder on \"emotion\"\n",
            "Cosine similarity from Universal sentence encoder for mood and emotion = [[0.77989013]]\n",
            "\n",
            "\u001b[1mword pair 58\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"comfort\" and \"safety\"\n",
            "Running universal sentence encoder on \"comfort\"\n",
            "Running universal sentence encoder on \"safety\"\n",
            "Cosine similarity from Universal sentence encoder for comfort and safety = [[0.56179284]]\n",
            "\n",
            "\u001b[1mword pair 59\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"activity\" and \"movement\"\n",
            "Running universal sentence encoder on \"activity\"\n",
            "Running universal sentence encoder on \"movement\"\n",
            "Cosine similarity from Universal sentence encoder for activity and movement = [[0.53950975]]\n",
            "\n",
            "\u001b[1mword pair 60\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"adult\" and \"guardian\"\n",
            "Running universal sentence encoder on \"adult\"\n",
            "Running universal sentence encoder on \"guardian\"\n",
            "Cosine similarity from Universal sentence encoder for adult and guardian = [[0.38469921]]\n",
            "\n",
            "\u001b[1mword pair 61\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"nurse\" and \"scientist\"\n",
            "Running universal sentence encoder on \"nurse\"\n",
            "Running universal sentence encoder on \"scientist\"\n",
            "Cosine similarity from Universal sentence encoder for nurse and scientist = [[0.48110114]]\n",
            "\n",
            "\u001b[1mword pair 62\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"hip\" and \"lip\"\n",
            "Running universal sentence encoder on \"hip\"\n",
            "Running universal sentence encoder on \"lip\"\n",
            "Cosine similarity from Universal sentence encoder for hip and lip = [[0.48654319]]\n",
            "\n",
            "\u001b[1mword pair 63\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bed\" and \"couch\"\n",
            "Running universal sentence encoder on \"bed\"\n",
            "Running universal sentence encoder on \"couch\"\n",
            "Cosine similarity from Universal sentence encoder for bed and couch = [[0.71440753]]\n",
            "\n",
            "\u001b[1mword pair 64\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"pipe\" and \"cigar\"\n",
            "Running universal sentence encoder on \"pipe\"\n",
            "Running universal sentence encoder on \"cigar\"\n",
            "Cosine similarity from Universal sentence encoder for pipe and cigar = [[0.72840089]]\n",
            "\n",
            "\u001b[1mword pair 65\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"cup\" and \"jar\"\n",
            "Running universal sentence encoder on \"cup\"\n",
            "Running universal sentence encoder on \"jar\"\n",
            "Cosine similarity from Universal sentence encoder for cup and jar = [[0.56287193]]\n",
            "\n",
            "\u001b[1mword pair 66\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"steeple\" and \"chapel\"\n",
            "Running universal sentence encoder on \"steeple\"\n",
            "Running universal sentence encoder on \"chapel\"\n",
            "Cosine similarity from Universal sentence encoder for steeple and chapel = [[0.74813154]]\n",
            "\n",
            "\u001b[1mword pair 67\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"bottle\" and \"container\"\n",
            "Running universal sentence encoder on \"bottle\"\n",
            "Running universal sentence encoder on \"container\"\n",
            "Cosine similarity from Universal sentence encoder for bottle and container = [[0.67109282]]\n",
            "\n",
            "\u001b[1mword pair 68\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"calf\" and \"bull\"\n",
            "Running universal sentence encoder on \"calf\"\n",
            "Running universal sentence encoder on \"bull\"\n",
            "Cosine similarity from Universal sentence encoder for calf and bull = [[0.43594493]]\n",
            "\n",
            "\u001b[1mword pair 69\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"dinner\" and \"chicken\"\n",
            "Running universal sentence encoder on \"dinner\"\n",
            "Running universal sentence encoder on \"chicken\"\n",
            "Cosine similarity from Universal sentence encoder for dinner and chicken = [[0.70112527]]\n",
            "\n",
            "\u001b[1mword pair 70\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"attention\" and \"awareness\"\n",
            "Running universal sentence encoder on \"attention\"\n",
            "Running universal sentence encoder on \"awareness\"\n",
            "Cosine similarity from Universal sentence encoder for attention and awareness = [[0.58921858]]\n",
            "\n",
            "\u001b[1mword pair 71\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"arithmetic\" and \"rhythm\"\n",
            "Running universal sentence encoder on \"arithmetic\"\n",
            "Running universal sentence encoder on \"rhythm\"\n",
            "Cosine similarity from Universal sentence encoder for arithmetic and rhythm = [[0.47935476]]\n",
            "\n",
            "\u001b[1mword pair 72\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"flower\" and \"endurance\"\n",
            "Running universal sentence encoder on \"flower\"\n",
            "Running universal sentence encoder on \"endurance\"\n",
            "Cosine similarity from Universal sentence encoder for flower and endurance = [[0.29797482]]\n",
            "\n",
            "\u001b[1mword pair 73\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"liquor\" and \"century\"\n",
            "Running universal sentence encoder on \"liquor\"\n",
            "Running universal sentence encoder on \"century\"\n",
            "Cosine similarity from Universal sentence encoder for liquor and century = [[0.38774603]]\n",
            "\n",
            "\u001b[1mword pair 74\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"course\" and \"stomach\"\n",
            "Running universal sentence encoder on \"course\"\n",
            "Running universal sentence encoder on \"stomach\"\n",
            "Cosine similarity from Universal sentence encoder for course and stomach = [[0.29764914]]\n",
            "\n",
            "\u001b[1mword pair 75\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"apple\" and \"sunshine\"\n",
            "Running universal sentence encoder on \"apple\"\n",
            "Running universal sentence encoder on \"sunshine\"\n",
            "Cosine similarity from Universal sentence encoder for apple and sunshine = [[0.34463196]]\n",
            "\n",
            "\u001b[1mword pair 76\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"rice\" and \"boy\"\n",
            "Running universal sentence encoder on \"rice\"\n",
            "Running universal sentence encoder on \"boy\"\n",
            "Cosine similarity from Universal sentence encoder for rice and boy = [[0.26355598]]\n",
            "\n",
            "\u001b[1mword pair 77\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"stomach\" and \"bedroom\"\n",
            "Running universal sentence encoder on \"stomach\"\n",
            "Running universal sentence encoder on \"bedroom\"\n",
            "Cosine similarity from Universal sentence encoder for stomach and bedroom = [[0.30915861]]\n",
            "\n",
            "\u001b[1mword pair 78\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"multiply\" and \"divide\"\n",
            "Running universal sentence encoder on \"multiply\"\n",
            "Running universal sentence encoder on \"divide\"\n",
            "Cosine similarity from Universal sentence encoder for multiply and divide = [[0.74252377]]\n",
            "\n",
            "\u001b[1mword pair 79\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"choose\" and \"elect\"\n",
            "Running universal sentence encoder on \"choose\"\n",
            "Running universal sentence encoder on \"elect\"\n",
            "Cosine similarity from Universal sentence encoder for choose and elect = [[0.30976689]]\n",
            "\n",
            "\u001b[1mword pair 80\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"advise\" and \"recommend\"\n",
            "Running universal sentence encoder on \"advise\"\n",
            "Running universal sentence encoder on \"recommend\"\n",
            "Cosine similarity from Universal sentence encoder for advise and recommend = [[0.5882619]]\n",
            "\n",
            "\u001b[1mword pair 81\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"understand\" and \"know\"\n",
            "Running universal sentence encoder on \"understand\"\n",
            "Running universal sentence encoder on \"know\"\n",
            "Cosine similarity from Universal sentence encoder for understand and know = [[0.73980853]]\n",
            "\n",
            "\u001b[1mword pair 82\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"come\" and \"attend\"\n",
            "Running universal sentence encoder on \"come\"\n",
            "Running universal sentence encoder on \"attend\"\n",
            "Cosine similarity from Universal sentence encoder for come and attend = [[0.54671534]]\n",
            "\n",
            "\u001b[1mword pair 83\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"arrive\" and \"come\"\n",
            "Running universal sentence encoder on \"arrive\"\n",
            "Running universal sentence encoder on \"come\"\n",
            "Cosine similarity from Universal sentence encoder for arrive and come = [[0.59394721]]\n",
            "\n",
            "\u001b[1mword pair 84\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"take\" and \"carry\"\n",
            "Running universal sentence encoder on \"take\"\n",
            "Running universal sentence encoder on \"carry\"\n",
            "Cosine similarity from Universal sentence encoder for take and carry = [[0.47463889]]\n",
            "\n",
            "\u001b[1mword pair 85\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"lose\" and \"keep\"\n",
            "Running universal sentence encoder on \"lose\"\n",
            "Running universal sentence encoder on \"keep\"\n",
            "Cosine similarity from Universal sentence encoder for lose and keep = [[0.59587151]]\n",
            "\n",
            "\u001b[1mword pair 86\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"forget\" and \"know\"\n",
            "Running universal sentence encoder on \"forget\"\n",
            "Running universal sentence encoder on \"know\"\n",
            "Cosine similarity from Universal sentence encoder for forget and know = [[0.68886524]]\n",
            "\n",
            "\u001b[1mword pair 87\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"pretend\" and \"imagine\"\n",
            "Running universal sentence encoder on \"pretend\"\n",
            "Running universal sentence encoder on \"imagine\"\n",
            "Cosine similarity from Universal sentence encoder for pretend and imagine = [[0.66324907]]\n",
            "\n",
            "\u001b[1mword pair 88\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"say\" and \"participate\"\n",
            "Running universal sentence encoder on \"say\"\n",
            "Running universal sentence encoder on \"participate\"\n",
            "Cosine similarity from Universal sentence encoder for say and participate = [[0.49971228]]\n",
            "\n",
            "\u001b[1mword pair 89\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"ask\" and \"plead\"\n",
            "Running universal sentence encoder on \"ask\"\n",
            "Running universal sentence encoder on \"plead\"\n",
            "Cosine similarity from Universal sentence encoder for ask and plead = [[0.42394544]]\n",
            "\n",
            "\u001b[1mword pair 90\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"give\" and \"put\"\n",
            "Running universal sentence encoder on \"give\"\n",
            "Running universal sentence encoder on \"put\"\n",
            "Cosine similarity from Universal sentence encoder for give and put = [[0.78829235]]\n",
            "\n",
            "\u001b[1mword pair 91\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"understand\" and \"forgive\"\n",
            "Running universal sentence encoder on \"understand\"\n",
            "Running universal sentence encoder on \"forgive\"\n",
            "Cosine similarity from Universal sentence encoder for understand and forgive = [[0.51262141]]\n",
            "\n",
            "\u001b[1mword pair 92\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"accept\" and \"forgive\"\n",
            "Running universal sentence encoder on \"accept\"\n",
            "Running universal sentence encoder on \"forgive\"\n",
            "Cosine similarity from Universal sentence encoder for accept and forgive = [[0.5470676]]\n",
            "\n",
            "\u001b[1mword pair 93\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"arrange\" and \"organize\"\n",
            "Running universal sentence encoder on \"arrange\"\n",
            "Running universal sentence encoder on \"organize\"\n",
            "Cosine similarity from Universal sentence encoder for arrange and organize = [[0.53682741]]\n",
            "\n",
            "\u001b[1mword pair 94\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"pretend\" and \"seem\"\n",
            "Running universal sentence encoder on \"pretend\"\n",
            "Running universal sentence encoder on \"seem\"\n",
            "Cosine similarity from Universal sentence encoder for pretend and seem = [[0.62766681]]\n",
            "\n",
            "\u001b[1mword pair 95\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"try\" and \"think\"\n",
            "Running universal sentence encoder on \"try\"\n",
            "Running universal sentence encoder on \"think\"\n",
            "Cosine similarity from Universal sentence encoder for try and think = [[0.69749588]]\n",
            "\n",
            "\u001b[1mword pair 96\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"leave\" and \"appear\"\n",
            "Running universal sentence encoder on \"leave\"\n",
            "Running universal sentence encoder on \"appear\"\n",
            "Cosine similarity from Universal sentence encoder for leave and appear = [[0.5257702]]\n",
            "\n",
            "\u001b[1mword pair 97\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"put\" and \"hang\"\n",
            "Running universal sentence encoder on \"put\"\n",
            "Running universal sentence encoder on \"hang\"\n",
            "Cosine similarity from Universal sentence encoder for put and hang = [[0.59805026]]\n",
            "\n",
            "\u001b[1mword pair 98\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"say\" and \"verify\"\n",
            "Running universal sentence encoder on \"say\"\n",
            "Running universal sentence encoder on \"verify\"\n",
            "Cosine similarity from Universal sentence encoder for say and verify = [[0.38359372]]\n",
            "\n",
            "\u001b[1mword pair 99\u001b[0m\n",
            "Embedding this pair from Simlex_999: \"absorb\" and \"possess\"\n",
            "Running universal sentence encoder on \"absorb\"\n",
            "Running universal sentence encoder on \"possess\"\n",
            "Cosine similarity from Universal sentence encoder for absorb and possess = [[0.62820548]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK32zTCP1Zm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simlex_out_USE_2 = copy.deepcopy(simlex_out_USE)\n",
        "simlex_out_USE_2 = simlex_out_USE_2.iloc[50:100,:]\n",
        "simlex_out_USE_2.to_csv('universal_sentence_encoder_2.csv',sep='\\t')\n",
        "files.download('universal_sentence_encoder_2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NitX31RhGqH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}